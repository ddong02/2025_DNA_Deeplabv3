# WandB Sweep Configuration for Learning Rate and Target Max Ratio Optimization
# This configuration optimizes two key hyperparameters for fine-tuning with Weighted CE Loss

program: my_train.py
method: bayes
metric:
  goal: maximize
  name: "[Val] Mean IoU"
project: "deeplabv3-segmentation"

# Fixed parameters (not optimized)
parameters:
  # Dataset and model parameters
  dataset:
    value: "dna2025dataset"
  data_root:
    value: "./datasets/data"
  model:
    value: "deeplabv3_mobilenet"
  num_classes:
    value: 19
  pretrained_num_classes:
    value: 19
  output_stride:
    value: 16
  
  # Training parameters
  epochs:
    value: 30  # Reduced for faster sweep
  unfreeze_epoch:
    value: 16
  batch_size:
    value: 4
  val_batch_size:
    value: 4
  crop_size:
    value: 1024
  # experiment_name: removed to allow dynamic generation
  # Checkpoint (required)
  ckpt:
    value: "./checkpoints/ce_only2/best_model.pth"
  
  # WandB settings
  wandb_project:
    value: "deeplabv3-segmentation"
  # wandb_name: removed to allow dynamic generation
  wandb_tags:
    value: "sweep,lr_optimization,class_weights"
  
  # Early stopping parameters
  early_stop_patience:
    value: 8
  early_stop_min_delta:
    value: 0.001
  early_stop_metric:
    value: "Mean IoU"
  
  # Fixed training parameters
  weight_decay:
    value: 0.0001
  random_seed:
    value: 1
  print_interval:
    value: 10
  
  # Class weights parameters
  class_weights_file:
    value: "class_weights/dna2025dataset_sqrt_inv_freq_nc19.pth"
  
  # Dataset subset for faster testing
  subset_ratio:
    value: 1.0

  # Parameters to optimize
  lr:
    distribution: log_uniform_values
    min: 0.000001    # 1e-6 (더 넓은 범위)
    max: 0.0001     # 1e-4 (100배 범위)
  
  target_max_ratio:
    distribution: uniform
    min: 5.0
    max: 15.0