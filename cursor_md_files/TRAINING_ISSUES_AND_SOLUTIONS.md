# Loss & mIoU ë™ì‹œ í•˜ë½ ë¬¸ì œ: ì›ì¸ê³¼ í•´ê²°ì±…

## ğŸš¨ ë¬¸ì œ ìƒí™©

```
Combined Loss + Class Weights ì ìš© í›„:

Epoch 1: Loss=0.8, mIoU=45%
Epoch 2: Loss=0.7, mIoU=42% âš ï¸
Epoch 3: Loss=0.6, mIoU=38% âš ï¸
Epoch 5: Loss=0.5, mIoU=32% ğŸ”´ ë¬¸ì œ!

ì˜ˆìƒ: Loss â†“, mIoU â†‘
ì‹¤ì œ: Loss â†“, mIoU â†“ (ë‘˜ ë‹¤ í•˜ë½!)
```

**ì´ê²ƒì€ ëª¨ë¸ì´ ì˜ëª» í•™ìŠµë˜ê³  ìˆë‹¤ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤!**

---

## ğŸ” ì›ì¸ ë¶„ì„

### ì›ì¸ 1: ê·¹ë‹¨ì ì¸ Class Weights (ê°€ì¥ í”í•¨!) â­â­â­

#### ë¬¸ì œ
```python
# inverse_freq ì‚¬ìš© ì‹œ
class_weights ê³„ì‚° ê²°ê³¼:
Class 0 (Road, ë§ìŒ):      weight = 0.1
Class 11 (Pedestrian, ì ìŒ): weight = 500.0

â†’ 500ë°° ì°¨ì´!

í›ˆë ¨ ì¤‘:
- Pedestrian í”½ì…€ 1ê°œ ì˜ëª» = loss Ã— 500
- Road í”½ì…€ 100ê°œ ì˜ëª» = loss Ã— 10

â†’ ëª¨ë¸ì´ Pedestrianë§Œ ì‹ ê²½ì“°ê³  Road ë¬´ì‹œ
â†’ mIoU í•˜ë½!
```

#### ì™œ LossëŠ” ë‚®ì•„ì§€ëŠ”ê°€?
```
ëª¨ë¸ì˜ ì „ëµ:
"Pedestrian(ê°€ì¤‘ì¹˜ 500)ë§Œ ë§ì¶”ë©´ lossê°€ ë‚®ì•„ì§„ë‹¤"

ì‹¤ì œ í•™ìŠµ:
- Pedestrian: IoU 80% (ì¢‹ìŒ)
- Road: IoU 30% (ë‚˜ì¨)
- Car: IoU 40% (ë‚˜ì¨)

Weighted Loss: ë‚®ìŒ (Pedestrian ê°€ì¤‘ì¹˜ í¼)
Mean IoU: ë‚®ìŒ (Road, Car ë§ê°€ì§)

â†’ Lossì™€ ì‹¤ì œ ì„±ëŠ¥ì˜ ë¶ˆì¼ì¹˜!
```

#### í•´ê²°ì±…
```python
# âŒ ë„ˆë¬´ ê·¹ë‹¨ì 
weights = calculate_class_weights(method='inverse_freq')

# âœ… ì™„í™”ëœ ê°€ì¤‘ì¹˜ (ì¶”ì²œ!)
weights = calculate_class_weights(method='sqrt_inv_freq')

# ì˜ˆì‹œ ë¹„êµ:
# inverse_freq:    [0.1, 500.0] (5000ë°° ì°¨ì´)
# sqrt_inv_freq:   [0.7, 22.6]  (32ë°° ì°¨ì´)
```

---

### ì›ì¸ 2: Dice Lossì˜ ì´ˆê¸° ë¶ˆì•ˆì •ì„± â­â­

#### ë¬¸ì œ
```python
Dice = (2 Ã— intersection) / (prediction + ground_truth)

ì´ˆê¸° í›ˆë ¨:
- ì˜ˆì¸¡ì´ ë§¤ìš° ë¶ˆí™•ì‹¤ (randomì— ê°€ê¹Œì›€)
- intersectionì´ ê±°ì˜ 0ì— ê°€ê¹Œì›€
- Denominatorë„ ë¶ˆì•ˆì •

ì˜ˆì‹œ:
Dice = (2 Ã— 0.001) / (0.1 + 0.15) = 0.008
Gradient: ë§¤ìš° í° ê°’ ë˜ëŠ” NaN

â†’ í›ˆë ¨ í­ë°œ ë˜ëŠ” ë°œì‚°!
```

#### ì‹¤ì œ ë¡œê·¸ ì˜ˆì‹œ
```
Epoch 1:
  CE Loss: 0.8 (ì•ˆì •)
  Dice Loss: 0.95 (ë§¤ìš° í¼)
  Combined: 0.6Ã—0.8 + 0.4Ã—0.95 = 0.86

Epoch 2:
  CE Loss: 0.7
  Dice Loss: 0.98 (ë” ì»¤ì§!)
  Combined: 0.88 (ì¦ê°€!)

â†’ Diceê°€ í•™ìŠµì„ ë°©í•´
```

#### í•´ê²°ì±…

**Solution 1: Warm-up (ì¶”ì²œ!)**
```python
# ì´ˆê¸°ì—ëŠ” CEë§Œ, ë‚˜ì¤‘ì— Dice ì¶”ê°€
def get_loss_weights(epoch, warmup_epochs=5):
    if epoch < warmup_epochs:
        # Warm-up: CEë§Œ
        ce_weight = 1.0
        dice_weight = 0.0
    else:
        # ì ì§„ì ìœ¼ë¡œ Dice ì¶”ê°€
        progress = min((epoch - warmup_epochs) / 10, 1.0)
        ce_weight = 0.6 + 0.4 * (1 - progress)
        dice_weight = 0.4 * progress
    
    return ce_weight, dice_weight

# í›ˆë ¨ ì¤‘:
ce_w, dice_w = get_loss_weights(epoch)
criterion = CombinedLoss(ce_weight=ce_w, dice_weight=dice_w, ...)

# Epoch 1-5: CE 100%
# Epoch 6-15: CE ì ì§„ì  ê°ì†Œ, Dice ì ì§„ì  ì¦ê°€
# Epoch 16+: CE 60%, Dice 40%
```

**Solution 2: Smooth ì¦ê°€**
```python
# ì´ˆê¸°ì— ë” í° smooth factor
def get_dice_smooth(epoch):
    if epoch < 10:
        return 10.0  # í° smooth (ì•ˆì •)
    elif epoch < 20:
        return 5.0
    else:
        return 1.0   # í‘œì¤€ smooth

smooth = get_dice_smooth(epoch)
criterion = CombinedLoss(smooth=smooth, ...)
```

**Solution 3: Square Denominator**
```python
# Dice Lossë¥¼ ë” ì•ˆì •ì ìœ¼ë¡œ
criterion = CombinedLoss(
    square_denominator=True,  # ì•ˆì •ì„± í–¥ìƒ!
    ...
)

# ìˆ˜ì‹:
# ê¸°ë³¸: (2Ã—intersection) / (pred + target)
# Square: (2Ã—intersection) / (predÂ² + targetÂ²)
# â†’ Gradientê°€ ë” ì•ˆì •ì 
```

---

### ì›ì¸ 3: Learning Rate ë¶€ì ì ˆ â­â­â­

#### ë¬¸ì œ
```python
Baseline (CE only):
- Loss scale: ~0.5
- Gradient scale: ~0.01
- LR: 1e-5 (ì í•©)

Combined + Weights:
- Loss scale: ~2.0 (4ë°° ì¦ê°€!)
- Gradient scale: ~0.05 (5ë°° ì¦ê°€!)
- LR: 1e-5 (ì—¬ì „íˆ ë™ì¼)

â†’ Learning rateê°€ ë„ˆë¬´ í¼!
â†’ Overshoot ë°œìƒ
â†’ ë°œì‚°
```

#### ì‹¤ì œ ì˜ˆì‹œ
```
LR=1e-5 (ë„ˆë¬´ í¼):
Epoch 1: Loss=0.8, mIoU=45%
Epoch 2: Loss=0.6, mIoU=40% (overshoot)
Epoch 3: Loss=0.9, mIoU=35% (ë°œì‚°)

LR=5e-6 (ì ì ˆ):
Epoch 1: Loss=0.8, mIoU=45%
Epoch 2: Loss=0.75, mIoU=46% (ì•ˆì •ì  ê°œì„ )
Epoch 3: Loss=0.7, mIoU=48%
```

#### í•´ê²°ì±…

**Solution 1: Learning Rate ê°ì†Œ (ì¦‰ê° íš¨ê³¼)**
```python
# Baseline LR
baseline_lr = 1e-5

# Combined Loss ì‚¬ìš© ì‹œ
adjusted_lr = baseline_lr * 0.5  # ë˜ëŠ” 0.3
# = 5e-6 ë˜ëŠ” 3e-6

optimizer = torch.optim.SGD(
    params=model.parameters(),
    lr=adjusted_lr,  # ê°ì†Œëœ LR
    momentum=0.9
)
```

**Solution 2: Gradient Clipping (ê°•ë ¥ ì¶”ì²œ!)**
```python
# Forward & Backward
loss = criterion(outputs, labels)
loss.backward()

# Gradient Clipping (ì¶”ê°€!)
torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_norm=1.0  # gradient norm ì œí•œ
)

optimizer.step()

# íš¨ê³¼:
# - Gradient explosion ë°©ì§€
# - í›ˆë ¨ ì•ˆì •í™”
# - ë” í° LR ì‚¬ìš© ê°€ëŠ¥
```

**Solution 3: LR Warm-up**
```python
def get_warmup_lr(epoch, base_lr, warmup_epochs=5):
    if epoch < warmup_epochs:
        # ì„ í˜•ìœ¼ë¡œ ì¦ê°€
        return base_lr * (epoch + 1) / warmup_epochs
    return base_lr

# í›ˆë ¨ ì¤‘:
current_lr = get_warmup_lr(epoch, base_lr=5e-6)
for param_group in optimizer.param_groups:
    param_group['lr'] = current_lr

# Epoch 1: LR=1e-6
# Epoch 2: LR=2e-6
# Epoch 5: LR=5e-6
# Epoch 6+: LR=5e-6 (ìœ ì§€)
```

---

### ì›ì¸ 4: Loss Scale ë³€í™” â­â­

#### ë¬¸ì œ
```python
Baseline (CE only):
Loss = 0.5 (ì ì ˆí•œ ê·œëª¨)

Combined (CE + Dice):
CE Loss = 0.5
Dice Loss = 0.8
Combined = 0.6Ã—0.5 + 0.4Ã—0.8 = 0.62

ë¬¸ì œì—†ì–´ ë³´ì´ì§€ë§Œ...

ê°€ì¤‘ì¹˜ ì¶”ê°€:
CE Loss (weighted) = 2.5 (5ë°° ì¦ê°€!)
Dice Loss (weighted) = 1.5 (2ë°° ì¦ê°€!)
Combined = 0.6Ã—2.5 + 0.4Ã—1.5 = 2.1

â†’ Loss ê·œëª¨ê°€ 4ë°° ì¦ê°€!
â†’ Gradient í­ë°œ
```

#### í•´ê²°ì±…

**Solution 1: Loss Normalization**
```python
class NormalizedCombinedLoss(nn.Module):
    def __init__(self, ce_weight=0.6, dice_weight=0.4, ...):
        super().__init__()
        self.ce_weight = ce_weight
        self.dice_weight = dice_weight
        self.ce_loss = nn.CrossEntropyLoss(...)
        self.dice_loss = DiceLoss(...)
        
        # Moving average for normalization
        self.register_buffer('ce_scale', torch.tensor(1.0))
        self.register_buffer('dice_scale', torch.tensor(1.0))
        self.momentum = 0.99
    
    def forward(self, logits, targets):
        ce = self.ce_loss(logits, targets)
        dice = self.dice_loss(logits, targets)
        
        # Update scales (EMA)
        with torch.no_grad():
            self.ce_scale = self.momentum * self.ce_scale + \
                           (1-self.momentum) * ce.detach()
            self.dice_scale = self.momentum * self.dice_scale + \
                             (1-self.momentum) * dice.detach()
        
        # Normalize
        ce_norm = ce / (self.ce_scale + 1e-8)
        dice_norm = dice / (self.dice_scale + 1e-8)
        
        # Combine
        total = self.ce_weight * ce_norm + self.dice_weight * dice_norm
        
        return total
```

**Solution 2: ê°€ì¤‘ì¹˜ ì •ê·œí™” í™•ì¸**
```python
# calculate_class_weights.pyì—ì„œ
class_weights = class_weights / np.mean(class_weights)

# í™•ì¸
print(f"Weight mean: {class_weights.mean():.4f}")  # 1.0ì´ì–´ì•¼ í•¨
print(f"Weight std: {class_weights.std():.4f}")
print(f"Weight range: [{class_weights.min():.4f}, {class_weights.max():.4f}]")

# ë„ˆë¬´ ê·¹ë‹¨ì ì´ë©´ clip
class_weights = np.clip(class_weights, 0.1, 10.0)  # 100ë°° ì°¨ì´ë¡œ ì œí•œ
```

---

### ì›ì¸ 5: Batch Size ë¬¸ì œ â­

#### ë¬¸ì œ
```python
Dice LossëŠ” batch ë‚´ í†µê³„ ì‚¬ìš©

ì‘ì€ batch (ì˜ˆ: batch_size=2):
- í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜ê°€ ë§¤ìš° ì ìŒ
- ì¼ë¶€ í´ë˜ìŠ¤ê°€ batchì— ì—†ì„ ìˆ˜ ìˆìŒ
- Dice ê³„ì‚°ì´ ë¶ˆì•ˆì •

ì˜ˆì‹œ:
Batch 1: Roadë§Œ ìˆìŒ â†’ Dice for Pedestrian = NaN
Batch 2: Pedestrian 10í”½ì…€ â†’ Dice ë§¤ìš° ë¶ˆì•ˆì •
```

#### í•´ê²°ì±…

**Solution 1: Batch Size ì¦ê°€**
```python
# ê°€ëŠ¥í•˜ë©´ batch size ì¦ê°€
batch_size = 4  # ìµœì†Œ 4 ì´ìƒ ì¶”ì²œ
# ë˜ëŠ” 8, 16 (GPU ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)

# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: Gradient Accumulation
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps

for i, (images, labels) in enumerate(train_loader):
    loss = criterion(outputs, labels) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**Solution 2: Dice Loss ìˆ˜ì •**
```python
# Global Dice (ëª¨ë“  batch í•©ì³ì„œ ê³„ì‚°)
class GlobalDiceLoss(nn.Module):
    def forward(self, logits, targets):
        # Batch dimension ìœ ì§€í•˜ì—¬ í•©ì‚°
        probs = F.softmax(logits, dim=1)
        targets_one_hot = F.one_hot(targets, num_classes)
        
        # ì „ì²´ batchì— ëŒ€í•´ ê³„ì‚°
        intersection = (probs * targets_one_hot).sum(dim=(0,2,3))
        union = probs.sum(dim=(0,2,3)) + targets_one_hot.sum(dim=(0,2,3))
        
        dice = (2 * intersection + smooth) / (union + smooth)
        return 1 - dice.mean()
```

---

### ì›ì¸ 6: í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ë„ˆë¬´ ì‹¬í•¨ â­

#### ë¬¸ì œ
```python
ê·¹ë‹¨ì ì¸ ë¶ˆê· í˜•:
Class 0: 10,000,000 í”½ì…€ (99.9%)
Class 18: 100 í”½ì…€ (0.01%)

â†’ 10ë§Œë°° ì°¨ì´!

sqrt_inv_freqë¡œë„:
weight_0 = 1 / sqrt(0.999) = 1.0
weight_18 = 1 / sqrt(0.0001) = 100.0

â†’ ì—¬ì „íˆ 100ë°° ì°¨ì´
â†’ í›ˆë ¨ ë¶ˆì•ˆì •
```

#### í•´ê²°ì±…

**Solution 1: Weight Clipping**
```python
def calculate_class_weights_safe(..., max_weight_ratio=10.0):
    # ê¸°ì¡´ ê³„ì‚°
    class_weights = calculate_class_weights(...)
    
    # Normalize
    class_weights = class_weights / class_weights.mean()
    
    # Clip (ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨ ì œí•œ)
    min_weight = 1.0 / max_weight_ratio
    max_weight = max_weight_ratio
    
    class_weights = np.clip(class_weights, min_weight, max_weight)
    
    print(f"Weight range after clipping: [{class_weights.min():.2f}, {class_weights.max():.2f}]")
    print(f"Max ratio: {class_weights.max()/class_weights.min():.1f}x")
    
    return torch.FloatTensor(class_weights).to(device)

# ì‚¬ìš©
weights = calculate_class_weights_safe(
    dataset=train_dst,
    method='sqrt_inv_freq',
    max_weight_ratio=10.0  # ìµœëŒ€ 10ë°° ì°¨ì´
)
```

**Solution 2: ë‘ ë‹¨ê³„ ì ‘ê·¼**
```python
# Stage 1: ê°€ë²¼ìš´ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘
weights_stage1 = calculate_class_weights(method='sqrt_inv_freq')
weights_stage1 = np.clip(weights_stage1, 0.5, 2.0)  # 4ë°° ì œí•œ

# 20 epoch í›ˆë ¨...

# Stage 2: ë” ê°•í•œ ê°€ì¤‘ì¹˜ ì ìš©
weights_stage2 = calculate_class_weights(method='sqrt_inv_freq')
weights_stage2 = np.clip(weights_stage2, 0.2, 5.0)  # 25ë°° ì œí•œ

# ë‚˜ë¨¸ì§€ í›ˆë ¨...
```

---

## ğŸ¯ ì¢…í•© í•´ê²°ì±… (ì¶”ì²œ)

### ë°©ë²• 1: ì•ˆì „í•œ ì„¤ì • (ì´ˆë³´ì/ì•ˆì •ì„± ìš°ì„ )

```python
# 1. ì™„í™”ëœ ê°€ì¤‘ì¹˜
class_weights = calculate_class_weights(
    dataset=train_dst,
    method='sqrt_inv_freq',  # inverse_freq ì•„ë‹˜!
    num_classes=19,
    device=device
)

# 2. Weight clipping
class_weights = torch.clamp(class_weights, min=0.1, max=10.0)
print(f"Weight range: {class_weights.min():.2f} ~ {class_weights.max():.2f}")

# 3. Warm-up í¬í•¨ Combined Loss
class SafeCombinedLoss(nn.Module):
    def __init__(self, epoch, **kwargs):
        super().__init__()
        self.epoch = epoch
        
        # Warm-up logic
        if epoch < 5:
            ce_weight = 1.0
            dice_weight = 0.0
        elif epoch < 15:
            progress = (epoch - 5) / 10
            ce_weight = 1.0 - 0.4 * progress
            dice_weight = 0.4 * progress
        else:
            ce_weight = 0.6
            dice_weight = 0.4
        
        self.criterion = CombinedLoss(
            ce_weight=ce_weight,
            dice_weight=dice_weight,
            smooth=5.0 if epoch < 10 else 1.0,  # ì´ˆê¸° í° smooth
            class_weights=kwargs.get('class_weights'),
            square_denominator=True  # ì•ˆì •ì„±!
        )
    
    def forward(self, logits, targets):
        return self.criterion(logits, targets)

# 4. ê°ì†Œëœ Learning Rate
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=3e-6,  # ê¸°ì¡´ 1e-5ì—ì„œ ê°ì†Œ!
    momentum=0.9
)

# 5. Gradient Clipping
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()

# 6. í° Batch Size (ê°€ëŠ¥í•˜ë©´)
batch_size = 8  # ë˜ëŠ” gradient accumulation
```

---

### ë°©ë²• 2: ì ì§„ì  ì ìš© (ì¶”ì²œ!)

```python
# Phase 1: Class Weightsë§Œ (5 epochs)
criterion_phase1 = nn.CrossEntropyLoss(
    weight=class_weights_clipped,
    ignore_index=255
)
# LR = 5e-6
# â†’ ì•ˆì •ì„± í™•ì¸

# Phase 2: CE + ì•½í•œ Dice (10 epochs)
criterion_phase2 = CombinedLoss(
    ce_weight=0.8,
    dice_weight=0.2,  # ì•½í•˜ê²Œ ì‹œì‘
    class_weights=class_weights_clipped,
    smooth=5.0
)
# LR = 3e-6
# â†’ Dice íš¨ê³¼ í™•ì¸

# Phase 3: ìµœì¢… ë¹„ìœ¨ (ë‚˜ë¨¸ì§€)
criterion_phase3 = CombinedLoss(
    ce_weight=0.6,
    dice_weight=0.4,  # ìµœì¢… ë¹„ìœ¨
    class_weights=class_weights_clipped,
    smooth=1.0
)
# LR = 3e-6
# â†’ ìµœì¢… ì„±ëŠ¥
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í›ˆë ¨ ì¤‘ í™•ì¸ì‚¬í•­

```python
# 1. Loss ê°œë³„ í™•ì¸
print(f"CE Loss: {ce_loss.item():.4f}")
print(f"Dice Loss: {dice_loss.item():.4f}")
print(f"Combined: {total_loss.item():.4f}")

# 2. Gradient í™•ì¸
total_norm = 0
for p in model.parameters():
    if p.grad is not None:
        param_norm = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
total_norm = total_norm ** 0.5
print(f"Gradient norm: {total_norm:.4f}")

# ì •ìƒ: 0.1 ~ 10
# ë¬¸ì œ: > 100 (exploding) ë˜ëŠ” < 0.001 (vanishing)

# 3. í´ë˜ìŠ¤ë³„ IoU
for i, iou in enumerate(class_ious):
    print(f"Class {i}: {iou:.3f}")

# í™•ì¸: ëª¨ë“  í´ë˜ìŠ¤ê°€ ê³¨ê³ ë£¨ í•™ìŠµë˜ëŠ”ì§€

# 4. WandBì— ë¡œê¹…
wandb.log({
    'CE Loss': ce_loss,
    'Dice Loss': dice_loss,
    'Total Loss': total_loss,
    'Gradient Norm': total_norm,
    'Mean IoU': mean_iou,
    **{f'IoU/Class_{i}': iou for i, iou in enumerate(class_ious)}
})
```

---

## âš ï¸ ê²½ê³  ì‹ í˜¸

### ì¦‰ì‹œ í›ˆë ¨ì„ ë©ˆì¶°ì•¼ í•  ë•Œ:

```
ğŸš¨ Lossê°€ NaN ë˜ëŠ” Inf
â†’ Learning rate ë„ˆë¬´ í¼ ë˜ëŠ” gradient explosion

ğŸš¨ Lossê°€ ë°œì‚° (ê³„ì† ì¦ê°€)
â†’ Learning rate ë„ˆë¬´ í¼

ğŸš¨ Mean IoUê°€ 5 epoch ì—°ì† í•˜ë½
â†’ ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ê·¹ë‹¨ì 

ğŸš¨ íŠ¹ì • í´ë˜ìŠ¤ IoUê°€ 0ìœ¼ë¡œ ê³ ì •
â†’ í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì™„ì „íˆ ë¬´ì‹œë¨

ğŸš¨ Gradient norm > 1000
â†’ Gradient explosion ì§„í–‰ ì¤‘

â†’ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ì„¤ì • ì¬ì¡°ì •!
```

---

## âœ… ì„±ê³µ ì§€í‘œ

### ì˜¬ë°”ë¥´ê²Œ í•™ìŠµë˜ê³  ìˆë‹¤ë©´:

```
âœ… Loss ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œ
   Epoch 1: 0.8
   Epoch 5: 0.6
   Epoch 10: 0.5

âœ… Mean IoU ê¾¸ì¤€íˆ ì¦ê°€
   Epoch 1: 45%
   Epoch 5: 48%
   Epoch 10: 51%

âœ… ì†Œìˆ˜ í´ë˜ìŠ¤ IoU í–¥ìƒ
   Before: Pedestrian 15%
   After: Pedestrian 35% (+20%p)

âœ… ë‹¤ìˆ˜ í´ë˜ìŠ¤ IoU ìœ ì§€ ë˜ëŠ” ì•½ê°„ í–¥ìƒ
   Before: Road 85%
   After: Road 86% (+1%p)

âœ… Gradient norm ì•ˆì •ì  (0.1 ~ 10)

âœ… CE Lossì™€ Dice Loss ëª¨ë‘ ê°ì†Œ
```

---

## ğŸ¯ ìµœì¢… ì¶”ì²œ ì„¤ì •

```python
# ê°€ì¥ ì•ˆì „í•˜ê³  íš¨ê³¼ì ì¸ ì¡°í•©
class_weights = calculate_class_weights(
    method='sqrt_inv_freq',
    ...
)
class_weights = torch.clamp(class_weights, 0.1, 10.0)

criterion = CombinedLoss(
    ce_weight=0.7,           # CE ë¹„ì¤‘ ë†’ì„ (ì•ˆì •ì„±)
    dice_weight=0.3,         # Dice ë¹„ì¤‘ ë‚®ì¶¤ (ì´ˆê¸°)
    smooth=5.0,              # í° smooth (ì•ˆì •ì„±)
    class_weights=class_weights,
    square_denominator=True  # ì•ˆì •ì„±!
)

optimizer = torch.optim.SGD(
    lr=3e-6,  # ê°ì†Œëœ LR
    momentum=0.9
)

# í›ˆë ¨ ë£¨í”„
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
optimizer.step()
```

**ì´ ì„¤ì •ìœ¼ë¡œ ì‹œì‘í•˜ê³ , ì•ˆì •ì ì´ë©´ ì ì§„ì ìœ¼ë¡œ ì¡°ì •í•˜ì„¸ìš”!**

